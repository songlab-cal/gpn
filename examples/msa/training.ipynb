{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVs1eelGRwcp",
        "user_expressions": []
      },
      "source": [
        "# GPN-MSA: training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff_VhW6GciQ-",
        "outputId": "8061dc34-0e39-48f4-cace-70be7e872c70",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!pip install --quiet git+https://github.com/songlab-cal/gpn.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Data sources and output \n",
        "# see README for how to download and unzip MSA:\n",
        "# https://huggingface.co/datasets/songlab/multiz100way\n",
        "msa_path = \"../../analysis/human/results/msa/multiz100way/89/all.zarr\"\n",
        "training_windows_path = \"songlab/gpn-msa-sapiens-dataset\"\n",
        "output_path = \"checkpoints\"  # TODO: might need to do mkdir\n",
        "\n",
        "# Hyperparameters\n",
        "max_steps = 10 # just for demonstration, should be 30_000 in a real run\n",
        "loss_weight = 0.1\n",
        "seed = 42\n",
        "use_aux_features = True\n",
        "weight_conserved = True\n",
        "flip_nonconserved = True\n",
        "n_aux_features = 89 * 5 # (n_species * #{A,C,G,T,-})\n",
        "config_overrides = f\"n_aux_features={n_aux_features}\"  # here you can add e.g. ,hum_hidden_layers=8\n",
        "\n",
        "# System-specific config\n",
        "# The recommended total batch size is 2048\n",
        "# Since I'm running this notebook with 1 GPU, I'll put per_device_batch_size=512\n",
        "# and gradient_accumulation_steps=4\n",
        "n_gpu = 1\n",
        "per_device_batch_size = 512 # whatever fits in your GPU\n",
        "gradient_accumulation_steps = 4\n",
        "dataloader_num_workers = 8  # number of CPUs\n",
        "torchrun_path = \"/scratch/users/gbenegas/software/mambaforge/envs/gpn/bin/torchrun\"  # might just be \"torchrun\" in your system\n",
        "report_to = \"none\"  # we usually use wandb (might need to create an account)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/18/2023 17:11:38 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
            "/scratch/users/gbenegas/software/mambaforge/envs/gpn/lib/python3.11/site-packages/datasets/load.py:2088: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
            "  warnings.warn(\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['chrom', 'start', 'end', 'strand', 'phyloP', 'phastCons', 'lowercase'],\n",
            "        num_rows: 4415694\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['chrom', 'start', 'end', 'strand', 'phyloP', 'phastCons', 'lowercase'],\n",
            "        num_rows: 41586\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['chrom', 'start', 'end', 'strand', 'phyloP', 'phastCons', 'lowercase'],\n",
            "        num_rows: 55004\n",
            "    })\n",
            "})\n",
            "12/18/2023 17:11:40 - WARNING - __main__ - You are instantiating a new config instance from scratch.\n",
            "Loading MSA...\n",
            "Loading MSA... Done\n",
            "  0%|                                                    | 0/10 [00:00<?, ?it/s][rank0]:[2023-12-18 17:11:50,344] [0/0] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
            "[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "{'train_runtime': 64.0375, 'train_samples_per_second': 319.813, 'train_steps_per_second': 0.156, 'train_loss': 1.7629276275634767, 'epoch': 0.0}\n",
            "100%|███████████████████████████████████████████| 10/10 [01:04<00:00,  6.40s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =        0.0\n",
            "  train_loss               =     1.7629\n",
            "  train_runtime            = 0:01:04.03\n",
            "  train_samples_per_second =    319.813\n",
            "  train_steps_per_second   =      0.156\n",
            "100%|███████████████████████████████████████████| 82/82 [00:19<00:00,  4.19it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        0.0\n",
            "  eval_loss               =     1.5002\n",
            "  eval_runtime            = 0:00:21.76\n",
            "  eval_samples_per_second =   1910.332\n",
            "  eval_steps_per_second   =      3.767\n",
            "  perplexity              =     4.4828\n"
          ]
        }
      ],
      "source": [
        "!WANDB_PROJECT=GPN_MSA_SAPIENS_EXAMPLE {torchrun_path} --nproc_per_node={n_gpu} -m gpn.msa.train --do_train \\\n",
        "    --do_eval --fp16 --report_to {report_to} --prediction_loss_only True \\\n",
        "    --dataset_name {training_windows_path} \\\n",
        "    --msa_path {msa_path} \\\n",
        "    --run_name example1 --output_dir {output_path} \\\n",
        "    --soft_masked_loss_weight_train {loss_weight} \\\n",
        "    --soft_masked_loss_weight_evaluation {loss_weight} \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --optim adamw_torch --learning_rate 1e-4 --lr_scheduler_type cosine \\\n",
        "    --seed {seed} \\\n",
        "    --dataloader_num_workers {dataloader_num_workers} \\\n",
        "    --save_strategy steps --save_steps 5000 --evaluation_strategy steps \\\n",
        "    --eval_steps 5000 --logging_steps 5000 --max_steps {max_steps} \\\n",
        "    --warmup_steps 1000 --save_total_limit 1 --load_best_model_at_end \\\n",
        "    --model_type GPNRoFormer --config_overrides {config_overrides} \\\n",
        "    --use_aux_features {use_aux_features} \\\n",
        "    --weight_conserved {weight_conserved} \\\n",
        "    --flip_nonconserved {flip_nonconserved} \\\n",
        "    --remove_unused_columns False \\\n",
        "    --per_device_train_batch_size {per_device_batch_size} \\\n",
        "    --per_device_eval_batch_size {per_device_batch_size} \\\n",
        "    --gradient_accumulation_steps {gradient_accumulation_steps} \\\n",
        "    --torch_compile"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gpn",
      "language": "python",
      "name": "gpn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}